{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating topics from product reviews.\n",
    "I took a data science class this semester and quicky realized I needed to learn Python (shocking, I know). To help me learn the basics, I worked on generating common topics that appear in prodcut reviews. Each \"topic\" is a cluster of words that appear frequently and in the same contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data.\n",
    "The data I'm using is a subset of Amazon office product reviews from [here](http://jmcauley.ucsd.edu/data/amazon/). There are 53,258 reviews, but I only kept reviews less than 500 characters (to satistfy requirements for a class assignment). This left me with 24,305 reviews. Removing stop words (\"and\", \"the\", etc.) left 647,723 words to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "\n",
    "reviews = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open('amazon_short_reviews.csv') as inputfile:\n",
    "    for line in inputfile:\n",
    "        line_words = tokenizer.tokenize(unidecode.unidecode(line).lower()) \n",
    "        reviews.append([w for w in line_words if not w in stop_words])  # remove stop words\n",
    "words = [word for line in reviews for word in line]\n",
    "reviews = [x for x in reviews if x != []]  # remove empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24305\n",
      "647723\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating SIP scores.\n",
    "An \"SIP\" is a statistically improbable phrase. The SIP score tells you just how improbable a word or phrase is. For example, a word with SIP = 5 is used 5 times as often in the corpus of interest (aka, product reviews) as it is in everyday language. \n",
    "\n",
    "To find the SIP scores I first calculated the local frequency for each word as (number of occurances / total number of words), filtering out words that occured less than 10 times. I used the `wordfreq` library to find the baseline word frequencies.  Dividing local frequency by baseline frequency gives the SIP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "num_words = len(words)\n",
    "text = nltk.Text(words)\n",
    "uniq_words = list(set(words))\n",
    "word_pool = []\n",
    "word_SIP = []\n",
    "for word in uniq_words:\n",
    "    count = text.count(word)\n",
    "    if count < 10:\n",
    "        continue\n",
    "    word_pool.append(word)\n",
    "    freq = word_frequency(word, 'en')\n",
    "    if freq == 0:\n",
    "        freq = float(.00001)\n",
    "    word_SIP.append(count/num_words/freq)\n",
    "\n",
    "word_pool_sorted = [x for _,x in sorted(zip(word_SIP, word_pool), reverse = True)]\n",
    "word_SIP_sorted = word_SIP\n",
    "word_SIP_sorted.sort(reverse = True)\n",
    "SIPscoreslist = list(zip(word_pool_sorted, word_SIP_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('binder', 1289.0747265053337),\n",
       " ('folders', 886.1633999626201),\n",
       " ('sturdy', 807.2216685543215),\n",
       " ('cartridges', 715.1373118764491),\n",
       " ('pens', 678.9790593846693)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIPscoreslist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors with Word2Vec.\n",
    "asdfasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING: if you set min_count more than 1, you may get a word that has high SIP but then \n",
    "# wouldn't appear in your word vectors\n",
    "model = Word2Vec(reviews, size=300, window=5, min_count=1, workers=4)\n",
    "# another concern: what if you find a word that has high similarity, but is not included in the SIP scores?\n",
    "# SIP and WORD2VEC need to have same base words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['binder'], topn=5))\n",
    "print(model.wv.most_similar(positive=['pens'], topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
