{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating topics from product reviews.\n",
    "I took a data science class this semester and quickly realized I needed to learn Python (shocking, I know). To help me learn the basics and fulfill several class assignments, I worked on generating common topics that appear in product reviews. Each \"topic\" is a cluster of words that frequently appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data.\n",
    "The data I'm using is a subset of Amazon office product reviews from [here](http://jmcauley.ucsd.edu/data/amazon/). There are 53,258 reviews, but I only kept the first 10,000 reviews with less than 500 characters. Removing stop words (\"and\", \"the\", etc.) left 246,172 words to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "\n",
    "reviews = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open('amazon_short_reviews.csv') as inputfile:\n",
    "    for line in inputfile:\n",
    "        line_words = tokenizer.tokenize(unidecode.unidecode(line).lower()) \n",
    "        reviews.append([w for w in line_words if not w in stop_words])\n",
    "words = [word for line in reviews for word in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "246172\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating SIP scores.\n",
    "An \"SIP\" is a statistically improbable phrase. The SIP score tells you just how improbable a word or phrase is. For example, a word with SIP = 5 is used 5 times as often in the corpus of interest (aka, product reviews) as it is in everyday language. \n",
    "\n",
    "To find the SIP scores I first calculated the local frequency for each word (number of occurances/total number of words), filtering out words that occured less than 10 times. I used the `wordfreq` library to find the baseline word frequencies.  Dividing local frequency by baseline frequency gives the SIP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "num_words = len(words)\n",
    "text = nltk.Text(words)\n",
    "uniq_words = list(set(words))\n",
    "word_pool = []\n",
    "word_SIP = []\n",
    "for word in uniq_words:\n",
    "    count = text.count(word)\n",
    "    if count < 10:\n",
    "        continue\n",
    "    word_pool.append(word)\n",
    "    freq = word_frequency(word, 'en')\n",
    "    if freq == 0:\n",
    "        freq = float(.00001)\n",
    "    word_SIP.append(count/num_words/freq)\n",
    "\n",
    "word_pool_sorted = [x for _,x in sorted(zip(word_SIP, word_pool), reverse = True)]\n",
    "word_SIP_sorted = word_SIP\n",
    "word_SIP_sorted.sort(reverse = True)\n",
    "SIPscoreslist = list(zip(word_pool_sorted, word_SIP_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 5 words with the highest SIP scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('envelopes', 931.1233099801483),\n",
       " ('binder', 869.5694745520298),\n",
       " ('folders', 869.1542524751455),\n",
       " ('sturdy', 805.7274660321721),\n",
       " ('cartridges', 740.3234551354059)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIPscoreslist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings with Word2Vec.\n",
    "`Gensim` is a Python library created specifically for topic modeling. Within the library is the `Word2Vec` function which uses a neural network to represent each word as a vector of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(reviews, size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word vectors can be compared (using something like cosine similarity) to find words that are similar to each other. Because the word vectors were created from product reviews, \"similar\" words are words that are used in the same contexts. Here we list the 5 words most similar to \"envelopes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('seal', 0.9962739944458008), ('envelope', 0.9949904680252075), ('seals', 0.9942649602890015), ('sealing', 0.9939274191856384), ('packages', 0.9936246275901794)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['envelopes'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate nouns and adjectives.\n",
    "Next I used part-of-speech tagging (POS tagging) to separate our words into nouns and adjectives. (In this version, I'm separating into nouns and non-nouns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIP order is mantained\n",
    "from nltk import pos_tag\n",
    "word_pos = word_pool_sorted\n",
    "word_pos = pos_tag(word_pos)\n",
    "\n",
    "nounBucket = []\n",
    "adjBucket = []\n",
    "# for now, just call all non-nouns \"adjectives\"\n",
    "for word in word_pos:\n",
    "    if word[1][:2] == \"NN\":\n",
    "        nounBucket.append(word[0])\n",
    "    else:\n",
    "        adjBucket.append(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, POS tagging is not perfect. I would like to come back to this and explore other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['envelopes', 'folders', 'cartridges', 'pens', 'inks', 'printer', 'staples', 'cartridge', 'pencils', 'tabs']\n",
      "['binder', 'sturdy', 'ink', 'avery', '3m', 'flimsy', 'adhesive', 'refill', 'staple', 'durable']\n"
     ]
    }
   ],
   "source": [
    "print(nounBucket[:10])\n",
    "print(adjBucket[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the nouns.\n",
    "I used nouns with high SIP scores as cluster \"centers\". Any other noun that is similar enough to the cluster centers (according to some threshold) is included in the cluster. Clusters are designed to be exclusive, so words can only appear in one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordBank = nounBucket\n",
    "\n",
    "num_clust = 10\n",
    "similarity_thresh = 0.90\n",
    "SIP_thresh = 10.0\n",
    "\n",
    "nounClusters = []\n",
    "for i in range(num_clust):\n",
    "    parent_word = wordBank[0] # the first one has highest SIP\n",
    "    cluster = []\n",
    "    cluster.append(parent_word)\n",
    "    count = 0\n",
    "    for i,word in enumerate(wordBank[1:]):\n",
    "        if count < 9:\n",
    "            if model.wv.similarity(parent_word, word) > similarity_thresh:\n",
    "                if word_SIP_sorted[i+1] > SIP_thresh:\n",
    "                    cluster.append(word)\n",
    "                    count += 1\n",
    "        else:\n",
    "            break\n",
    "    nounClusters.append(cluster)\n",
    "    wordBank = [t for t in wordBank if t not in cluster] # remove current clustr from wordBank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first two noun clusters. Hopefully, the clusters represent a common topic from the product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['envelopes',\n",
       "  'folders',\n",
       "  'inks',\n",
       "  'staples',\n",
       "  'pencils',\n",
       "  'tabs',\n",
       "  'markers',\n",
       "  'scotch',\n",
       "  'labels',\n",
       "  'tape'],\n",
       " ['cartridges',\n",
       "  'printer',\n",
       "  'cartridge',\n",
       "  'printers',\n",
       "  'hp',\n",
       "  'amazon',\n",
       "  'brands',\n",
       "  'refills',\n",
       "  'epson',\n",
       "  'costco']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nounClusters[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add adjectives to topics.\n",
    "I added adjectives to the noun clusters to add meaning and interpretability. This was done by finding common bigrams that contain both an adjective and a noun from one of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "adjClusters = []\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "adjSet = set(adjBucket)\n",
    "num_adj = 10\n",
    "for cluster in nounClusters:\n",
    "    # make a copy of reviews, replace all cluster words with the parent_word of that cluster\n",
    "    parent_word = cluster[0]\n",
    "    similar_words = cluster[1:]\n",
    "    words_copy = [parent_word if w in similar_words else w for w in words]\n",
    "    \n",
    "    # now find bigrams with parent_word and all words in adjBucket\n",
    "    finder = BigramCollocationFinder.from_words(words_copy, window_size=5)\n",
    "    parent_filter = lambda *w: parent_word not in w\n",
    "    adj_filter = lambda w1, w2: adjSet.isdisjoint([w1, w2])\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_ngram_filter(parent_filter)\n",
    "    finder.apply_ngram_filter(adj_filter)\n",
    "    adj_temp = finder.nbest(bigram_measures.pmi, num_adj)\n",
    "    adj_temp = [pair[1] if pair[0] == parent_word else pair[0] for pair in adj_temp]\n",
    "    adjClusters.append(adj_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first two adjective clusters, which correspond to the first two noun clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['masking',\n",
       "  'packing',\n",
       "  'virtually',\n",
       "  'hanging',\n",
       "  'rounded',\n",
       "  'mechanical',\n",
       "  'pendaflex',\n",
       "  'smead',\n",
       "  'invisible',\n",
       "  'thermal'],\n",
       " ['www',\n",
       "  'refilled',\n",
       "  'genuine',\n",
       "  'refurbished',\n",
       "  'costly',\n",
       "  'lowest',\n",
       "  'canon',\n",
       "  'starter',\n",
       "  'remanufactured',\n",
       "  'died']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjClusters[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Topics\n",
    "Time to bring the noun clusters and adjective clusters together to form topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in range(num_clust):\n",
    "    if len(adjClusters[i]) != 0:\n",
    "        topicName = \" \".join([nounClusters[i][0], adjClusters[i][0]])\n",
    "        queryParams = [topicName, adjClusters[i][1:], nounClusters[i][1:]]\n",
    "    else:\n",
    "        topicName = nounClusters[i][0]\n",
    "        queryParams = [topicName, [], nounClusters[i][1:]]\n",
    "    topics.append(queryParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['envelopes masking',\n",
       "  ['packing',\n",
       "   'virtually',\n",
       "   'hanging',\n",
       "   'rounded',\n",
       "   'mechanical',\n",
       "   'pendaflex',\n",
       "   'smead',\n",
       "   'invisible',\n",
       "   'thermal'],\n",
       "  ['folders',\n",
       "   'inks',\n",
       "   'staples',\n",
       "   'pencils',\n",
       "   'tabs',\n",
       "   'markers',\n",
       "   'scotch',\n",
       "   'labels',\n",
       "   'tape']],\n",
       " ['cartridges www',\n",
       "  ['refilled',\n",
       "   'genuine',\n",
       "   'refurbished',\n",
       "   'costly',\n",
       "   'lowest',\n",
       "   'canon',\n",
       "   'starter',\n",
       "   'remanufactured',\n",
       "   'died'],\n",
       "  ['printer',\n",
       "   'cartridge',\n",
       "   'printers',\n",
       "   'hp',\n",
       "   'amazon',\n",
       "   'brands',\n",
       "   'refills',\n",
       "   'epson',\n",
       "   'costco']]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "I have a lot  more to learn about Python and topic modeling. The two main topics \"envelopes masking\" and \"cartridges www\" are not particularly helpful, although using the clusters as a search query may yield some interesting results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
