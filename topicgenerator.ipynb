{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating topics from product reviews.\n",
    "I took a data science class this semester and quickly realized I needed to learn Python (shocking, I know). To help me learn the basics and fulfill several class assignments, I worked on generating common topics that appear in product reviews. Each \"topic\" is a cluster of words that appear frequently, and in the same contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data.\n",
    "The data I'm using is a subset of Amazon office product reviews from [here](http://jmcauley.ucsd.edu/data/amazon/). There are 53,258 reviews, but I only kept the first 10,000 reviews with less than 500 characters. Removing stop words (\"and\", \"the\", etc.) left 246,172 words to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "\n",
    "reviews = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open('amazon_short_reviews.csv') as inputfile:\n",
    "    for line in inputfile:\n",
    "        line_words = tokenizer.tokenize(unidecode.unidecode(line).lower()) \n",
    "        reviews.append([w for w in line_words if not w in stop_words])\n",
    "words = [word for line in reviews for word in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "246172\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating SIP scores.\n",
    "An \"SIP\" is a statistically improbable phrase. The SIP score tells you just how improbable a word or phrase is. For example, a word with SIP = 5 is used 5 times as often in the corpus of interest (aka, product reviews) as it is in everyday language. \n",
    "\n",
    "To find the SIP scores I first calculated the local frequency for each word (number of occurances/total number of words), filtering out words that occured less than 10 times. I used the `wordfreq` library to find the baseline word frequencies.  Dividing local frequency by baseline frequency gives the SIP score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "num_words = len(words)\n",
    "text = nltk.Text(words)\n",
    "uniq_words = list(set(words))\n",
    "word_pool = []\n",
    "word_SIP = []\n",
    "for word in uniq_words:\n",
    "    count = text.count(word)\n",
    "    if count < 10:\n",
    "        continue\n",
    "    word_pool.append(word)\n",
    "    freq = word_frequency(word, 'en')\n",
    "    if freq == 0:\n",
    "        freq = float(.00001)\n",
    "    word_SIP.append(count/num_words/freq)\n",
    "\n",
    "word_pool_sorted = [x for _,x in sorted(zip(word_SIP, word_pool), reverse = True)]\n",
    "word_SIP_sorted = word_SIP\n",
    "word_SIP_sorted.sort(reverse = True)\n",
    "SIPscoreslist = list(zip(word_pool_sorted, word_SIP_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of product reviews, \"binder\" is the word with the highest SIP score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('envelopes', 931.1233099801483),\n",
       " ('binder', 869.5694745520298),\n",
       " ('folders', 869.1542524751455),\n",
       " ('sturdy', 805.7274660321721),\n",
       " ('cartridges', 740.3234551354059)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIPscoreslist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings with Word2Vec.\n",
    "asdfasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(reviews, size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mailing', 0.9965976476669312), ('packages', 0.9965600967407227), ('stock', 0.9963386058807373), ('sealing', 0.9961926937103271), ('professional', 0.9958876371383667)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['envelopes'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate nouns and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will keep the nouns and adjs in order of SIP score\n",
    "from nltk import pos_tag\n",
    "word_pos = word_pool_sorted\n",
    "word_pos = pos_tag(word_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some problems. Binder should be a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('envelopes', 'NNS'),\n",
       " ('binder', 'VBP'),\n",
       " ('folders', 'NNS'),\n",
       " ('sturdy', 'JJ'),\n",
       " ('cartridges', 'NNS'),\n",
       " ('pens', 'NNS'),\n",
       " ('ink', 'VBP'),\n",
       " ('inks', 'NNS'),\n",
       " ('printer', 'NN'),\n",
       " ('staples', 'NNS')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nounBucket = []\n",
    "adjBucket = []\n",
    "# for now, just call all non-nouns \"adjectives\"\n",
    "for word in word_pos:\n",
    "    if word[1][:2] == \"NN\":\n",
    "        nounBucket.append(word[0])\n",
    "    else:\n",
    "        adjBucket.append(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['envelopes', 'folders', 'cartridges', 'pens', 'inks', 'printer', 'staples', 'cartridge', 'pencils', 'tabs']\n",
      "['binder', 'sturdy', 'ink', 'avery', '3m', 'flimsy', 'adhesive', 'refill', 'staple', 'durable']\n"
     ]
    }
   ],
   "source": [
    "print(nounBucket[:10])\n",
    "print(adjBucket[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordBank = nounBucket\n",
    "\n",
    "num_clust = 10\n",
    "similarity_thresh = 0.90\n",
    "SIP_thresh = 10.0\n",
    "\n",
    "nounClusters = []\n",
    "for i in range(num_clust):\n",
    "    parent_word = wordBank[0] # the first one has highest SIP\n",
    "    cluster = []\n",
    "    cluster.append(parent_word)\n",
    "    count = 0\n",
    "    for i,word in enumerate(wordBank[1:]):\n",
    "        if count < 9:\n",
    "            if model.wv.similarity(parent_word, word) > similarity_thresh:\n",
    "                if word_SIP_sorted[i+1] > SIP_thresh:\n",
    "                    cluster.append(word)\n",
    "                    count += 1\n",
    "        else:\n",
    "            break\n",
    "    nounClusters.append(cluster)\n",
    "    wordBank = [t for t in wordBank if t not in cluster] # remove current clustr from wordBank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['envelopes',\n",
       "  'folders',\n",
       "  'inks',\n",
       "  'staples',\n",
       "  'pencils',\n",
       "  'tabs',\n",
       "  'markers',\n",
       "  'scotch',\n",
       "  'labels',\n",
       "  'tape'],\n",
       " ['cartridges',\n",
       "  'printer',\n",
       "  'cartridge',\n",
       "  'printers',\n",
       "  'hp',\n",
       "  'amazon',\n",
       "  'brands',\n",
       "  'epson',\n",
       "  'costco',\n",
       "  'xl'],\n",
       " ['pens',\n",
       "  'erase',\n",
       "  'nib',\n",
       "  'dries',\n",
       "  'colors',\n",
       "  'prints',\n",
       "  'bleed',\n",
       "  'odor',\n",
       "  'printing',\n",
       "  'refills'],\n",
       " ['stapler',\n",
       "  'gel',\n",
       "  'magnets',\n",
       "  'matte',\n",
       "  'flap',\n",
       "  'pricey',\n",
       "  'pencil',\n",
       "  'peel',\n",
       "  'notebooks',\n",
       "  'dispenser'],\n",
       " ['templates',\n",
       "  'calculator',\n",
       "  'notebook',\n",
       "  'pads',\n",
       "  'packaging',\n",
       "  'bulky',\n",
       "  'sheets',\n",
       "  'glare',\n",
       "  'thinner',\n",
       "  'inserts']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nounClusters[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add adjectives to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "adjClusters = []\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "adjSet = set(adjBucket)\n",
    "num_adj = 10\n",
    "for cluster in nounClusters:\n",
    "    # make a copy of reviews, replace all cluster words with the parent_word of that cluster\n",
    "    parent_word = cluster[0]\n",
    "    similar_words = cluster[1:]\n",
    "    words_copy = [parent_word if w in similar_words else w for w in words]\n",
    "    \n",
    "    # now find bigrams with parent_word and all words in adjBucket\n",
    "    finder = BigramCollocationFinder.from_words(words_copy, window_size=5)\n",
    "    parent_filter = lambda *w: parent_word not in w\n",
    "    adj_filter = lambda w1, w2: adjSet.isdisjoint([w1, w2])\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_ngram_filter(parent_filter)\n",
    "    finder.apply_ngram_filter(adj_filter)\n",
    "    adj_temp = finder.nbest(bigram_measures.pmi, num_adj)\n",
    "    adj_temp = [pair[1] if pair[0] == parent_word else pair[0] for pair in adj_temp]\n",
    "    adjClusters.append(adj_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['masking',\n",
       "  'packing',\n",
       "  'virtually',\n",
       "  'hanging',\n",
       "  'rounded',\n",
       "  'mechanical',\n",
       "  'pendaflex',\n",
       "  'smead',\n",
       "  'invisible',\n",
       "  'thermal'],\n",
       " ['www',\n",
       "  'refilled',\n",
       "  'genuine',\n",
       "  'refurbished',\n",
       "  'lowest',\n",
       "  'canon',\n",
       "  'starter',\n",
       "  'remanufactured',\n",
       "  'died',\n",
       "  'prime'],\n",
       " ['assorted',\n",
       "  'sakura',\n",
       "  'vibrant',\n",
       "  'fountain',\n",
       "  '36',\n",
       "  'dry',\n",
       "  'assortment',\n",
       "  'g2',\n",
       "  'scratchy',\n",
       "  'pilot'],\n",
       " ['commercial',\n",
       "  'cup',\n",
       "  'swingline',\n",
       "  'mechanical',\n",
       "  'drafting',\n",
       "  'matching',\n",
       "  'automatic',\n",
       "  'electric',\n",
       "  'reduced',\n",
       "  'sharpens'],\n",
       " ['graphing',\n",
       "  'spiral',\n",
       "  'legal',\n",
       "  'bound',\n",
       "  '150',\n",
       "  'solar',\n",
       "  'website',\n",
       "  '15',\n",
       "  '65',\n",
       "  'poly'],\n",
       " ['filler',\n",
       "  'insertable',\n",
       "  'mate',\n",
       "  'leaf',\n",
       "  'kodak',\n",
       "  'ruled',\n",
       "  'photo',\n",
       "  '24',\n",
       "  'copy',\n",
       "  'glossy'],\n",
       " ['sits',\n",
       "  'fellowes',\n",
       "  'shredding',\n",
       "  'rarely',\n",
       "  'build',\n",
       "  'closer',\n",
       "  'combined',\n",
       "  'hurt',\n",
       "  'spills',\n",
       "  'puts'],\n",
       " ['highest',\n",
       "  'as_li_tl',\n",
       "  'www',\n",
       "  'flawlessly',\n",
       "  'lesser',\n",
       "  'lowest',\n",
       "  'competitive',\n",
       "  'competitive',\n",
       "  'high',\n",
       "  'unbeatable'],\n",
       " ['band',\n",
       "  '3mm',\n",
       "  'pair',\n",
       "  'laminating',\n",
       "  '16',\n",
       "  'prevent',\n",
       "  'sliding',\n",
       "  'fellowes',\n",
       "  'applying',\n",
       "  'effectively'],\n",
       " ['knitting',\n",
       "  'roughly',\n",
       "  'heavyweight',\n",
       "  'loading',\n",
       "  'roughly',\n",
       "  'pale',\n",
       "  'patterns',\n",
       "  'twist',\n",
       "  'common',\n",
       "  'adds']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjClusters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in range(num_clust):\n",
    "    if len(adjClusters[i]) != 0:\n",
    "        topicName = \" \".join([nounClusters[i][0], adjClusters[i][0]])\n",
    "        queryParams = [topicName, adjClusters[i][1:], nounClusters[i][1:]]\n",
    "    else:\n",
    "        topicName = nounClusters[i][0]\n",
    "        queryParams = [topicName, [], nounClusters[i][1:]]\n",
    "    topics.append(queryParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['envelopes masking',\n",
       "  ['packing',\n",
       "   'virtually',\n",
       "   'hanging',\n",
       "   'rounded',\n",
       "   'mechanical',\n",
       "   'pendaflex',\n",
       "   'smead',\n",
       "   'invisible',\n",
       "   'thermal'],\n",
       "  ['folders',\n",
       "   'inks',\n",
       "   'staples',\n",
       "   'pencils',\n",
       "   'tabs',\n",
       "   'markers',\n",
       "   'scotch',\n",
       "   'labels',\n",
       "   'tape']],\n",
       " ['cartridges www',\n",
       "  ['refilled',\n",
       "   'genuine',\n",
       "   'refurbished',\n",
       "   'lowest',\n",
       "   'canon',\n",
       "   'starter',\n",
       "   'remanufactured',\n",
       "   'died',\n",
       "   'prime'],\n",
       "  ['printer',\n",
       "   'cartridge',\n",
       "   'printers',\n",
       "   'hp',\n",
       "   'amazon',\n",
       "   'brands',\n",
       "   'epson',\n",
       "   'costco',\n",
       "   'xl']]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
